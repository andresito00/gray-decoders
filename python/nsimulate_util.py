import argparse
from enum import Enum
import numpy as np
import scipy.io as io
import matplotlib.pyplot as plt
from typing import List, Optional
from neuron import SpikeDistribution


class NSimTune(Enum):
    tune = "tune"
    file = "file"
    synthetic = "synthetic"
    sim3_1 = "sim3_1"
    simulate = "simulate"


def build_args():
    parser = argparse.ArgumentParser(
        "A Neural Spike Train generator.",
        usage="python3 nsimulate --mode synthetic --rates 50 --intervals 2000 --bin-size 10 --num-trials 10 --rand EXP --show",
    )

    # main parameters
    parser.add_argument(
        "--mode",
        "-m",
        type=str,
        default="synthetic",
        choices=[e.value for e in NSimTune],
        help="synthetic spikes generated by prob. distribution or file input",
    )

    parser.add_argument(
        "--show",
        "-s",
        action="store_true",
        default=False,
        help="whether or not to show the plots",
    )

    parser.add_argument(
        "--dest-ip",
        "-ip",
        type=str,
        default="127.0.0.1",
        help="destination hostname for generated rasters",
    )

    parser.add_argument(
        "--dest-port",
        "-p",
        type=int,
        default=8808,
        help="destination port for generated rasters",
    )

    parser.add_argument(
        "--lat",
        "-l",
        action="store_true",
        default=False,
        help="whether or not to measure script latencies",
    )

    # file input parameters
    parser.add_argument(
        "--mat-file",
        "-mf",
        type=str,
        help="filename argument for a .matfile (hardcoded to 100x8 array of"
        "structs containing 2-D arrays)",
    )

    # tuning curve parameters
    parser.add_argument(
        "--dirs",
        "-d",
        nargs="+",
        default=[10, 20, 30],
        type=int,
        help="list of the potential reach directions in units of degrees",
    )

    parser.add_argument("--rates", "-r", nargs="+", type=int, help="rate in spikes/s")

    # probability distribution parameters
    parser.add_argument(
        "--type",
        "-t",
        type=str,
        default="Poisson",
        choices=["Poisson"],
        help="type of synthetic data",
    )

    parser.add_argument(
        "--shift",
        "-sh",
        type=int,
        default=20,
        help="dc shift or offset of the probability distribution",
    )

    parser.add_argument(
        "--dists", "-td", type=int, default=10, help="target distance in units of cm"
    )

    parser.add_argument(
        "--rand",
        "-rn",
        type=str,
        choices=[x.name for x in SpikeDistribution],
        help="type of random variable",
    )

    parser.add_argument(
        "--scale-factor", "-sf", type=int, help="GAMMA distribution scale factor"
    )

    parser.add_argument(
        "--pref-dir",
        "-pd",
        type=int,
        default=45,
        help="preferred target direction in units of degrees",
    )

    parser.add_argument(
        "--num-trials",
        "-n",
        type=int,
        default=10,
        help="the number of trials we'll generate for each target",
    )

    parser.add_argument(
        "--start-time",
        "-st",
        type=int,
        help="the number of trials we'll generate for each target",
    )

    # raster generation parameters
    parser.add_argument(
        "--intervals",
        "-dt",
        nargs="+",
        type=int,
        help="the intervals of the stimuli in units of milliseconds",
    )

    parser.add_argument(
        "--bin-size",
        "-b",
        type=int,
        default=10,
        help="the binning resolution at which to capture the millisecond data",
    )

    return parser.parse_args()


def show():
    plt.show()


def plot_cosine_model(k: np.ndarray, figure_number: int) -> None:
    theta = np.linspace(0, 2 * np.pi, 80)
    exp_firing_rates = k[0][0] + k[1][0] * np.sin(theta) + k[2][0] * np.cos(theta)
    plt.figure(figure_number)
    plt.plot(dirs, rates, "r*")
    plt.plot(np.rad2deg(theta), exp_firing_rates)
    plt.xlabel("angle (degrees)")
    plt.ylabel("firing rate (spikes/s)")
    plt.draw()


def plot_rasters(spike_trains: List[np.ndarray], figure_number: Optional[int]) -> None:
    plt.figure(figure_number)
    plt.eventplot(spike_trains)
    plt.xlabel("time (ms)")
    plt.ylabel("sensor")
    plt.draw()


def _mask_pad(array: np.ndarray, max_length: int) -> np.ndarray:
    """
    Utility to create a masked array that fills to max_length
    """
    ogLen = len(array)
    assert ogLen <= max_length
    masked_array = np.asarray(np.zeros(max_length))
    for i in range(0, max_length):
        masked_array[i] = array[i] if i < ogLen else np.ma.masked
    return masked_array


def generate_inter_spike_interval_hist(
    spike_trains: np.ndarray,
    figure_number: Optional[int],
    show_plots: bool = False,
) -> List[np.ndarray]:
    """
    Creates a histogram of the inter-spike intervals in the provided rasters
    """
    isi = []
    # max diff intervals length of the provided spike_trains
    max_length = len(max(spike_trains, key=len)) - 1
    for trial in spike_trains:
        intervals = np.diff(trial)
        current_length = len(intervals)
        masked_intervals = intervals
        if current_length < max_length:
            masked_intervals = _mask_pad(intervals, max_length)
        isi.append(masked_intervals)

    if show_plots:
        plt.figure(figure_number)
        plt.hist(isi)
        plt.xlabel("inter-spike interval (ms)")
        plt.ylabel("# spikes")
        plt.draw()

    return isi


def generate_spike_time_hist(
    spike_trains: np.ndarray,
    start_time_ms: int,
    duration_ms: np.ndarray,
    bin_size_ms: int,
    figure_number: Optional[int],
    show_plots: bool = False,
) -> np.ndarray:
    """
    Creates a histogram of the binned spike counts
    from neurons modeled by the Poisson process for a given bin size.
    """
    num_trials = len(spike_trains)
    start = start_time_ms
    sentinel = start + bin_size_ms
    idx = 0
    total_time = np.sum(duration_ms)
    end_time = total_time
    if start < 0:
        end_time += start
    num_bins = int(total_time / bin_size_ms)
    spike_counts = np.zeros(num_bins, dtype=float)
    # divide by number of trials and delta-t to get avg rate in hz
    trials_and_delta_t = bin_size_ms * 10 ** (-3) * num_trials
    while sentinel < end_time:
        # accumulate count of values between start and stop for this bin across all trials
        for trial in range(0, num_trials):
            curr_trial = spike_trains[trial]
            lb = curr_trial[curr_trial >= start]
            ub = curr_trial[curr_trial < sentinel]
            inter = np.intersect1d(lb, ub, assume_unique=True)
            spike_counts[idx] += inter.shape[0]
        # now we'll take the average for this bin across all trials
        # and slide our window forward
        spike_counts[idx] = spike_counts[idx] / trials_and_delta_t
        start += bin_size_ms
        sentinel += bin_size_ms
        idx += 1

    if show_plots:
        plt.figure(figure_number)
        plt.bar(range(0, num_bins), spike_counts, align="center")
        plt.xlabel(f"bins ({bin_size_ms}s of ms)")
        plt.ylabel("avg spike rate (hz)")
        plt.draw()

    return spike_counts


def parse_mat_file(mat_file: str):
    """
    think of matContents as a 2-D matrix where
    the rows are each trial
    the columns are each motor function target
    each cell in the matrix contains a structure with another 2-D matrix where
    the rows are each each neuron sensor
    the columns are millisecond, where 1 implies an action potential firing has been detected by that sensor
    """
    var_name = io.whosmat(mat_file)[0][0]
    mat_contents = io.loadmat(mat_file)
    plan_training_data = mat_contents[var_name]
    num_trials = plan_training_data.shape[0]  # m = rows
    num_targets = plan_training_data.shape[1]  # n = columns

    # dimensions will always be referred to as "m x n"

    trials = []
    for i in range(0, num_trials):
        targets = []
        for j in range(0, num_targets):
            # the 1st dimension of this internal structure contains the actual spike train data (a 2-D matrix)
            targets.append(plan_training_data[i, j][1])

        trials.append(targets)

    # convert the list of lists (of 2-D arrays) to a 4-dimensional numpy array such that
    # the dimensions are, in order, [trial, target, sensor, neural spike/ms]
    trials = np.asarray(trials)

    outputDir = "parser_output/"
    filename, extension = mat_file.split(".")

    # the following loops save all trial data to a .txt file for each target
    for n in range(0, num_targets):
        print("Now parsing target " + str(n) + "...")
        with open(outputDir + filename + "_target" + str(n) + ".txt", "w") as f:
            for m in range(0, num_trials - 1):
                curr_trial = trials[m, n, :, :]
                next_trial = trials[m + 1, n, :, :]
                curr_trial = np.concatenate((curr_trial, next_trial), axis=1)

            # the following is the current format that the meschach matrix file input function expects to receive.
            f.write(
                "Matrix: "
                + str(curr_trial.shape[0])
                + " by "
                + str(curr_trial.shape[1])
                + "\n"
            )

            # faster now that we've concatenated our training trials...
            np.savetxt(f, curr_trial, delimiter=" ", fmt="%i")
